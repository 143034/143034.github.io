---
title: 爬取动态网页
date: 2020-05-06 08:52:33
tags:
- scrapy
categories:
- scrapy


---

# 使用Splash渲染引擎 #

## 一、概念 ##


> Splash是Scrapy官方推荐的JavaScript渲染引擎，它是使用Webkit开发的轻量级无界面浏览器，提供基于HTTP接口的JavaScript渲染服务。

## 二、使用 ##

### 安装splash ###
	apt-get install docker
	docker pull scrapinghub/splash

### 开启服务 ###
	docker run -p 8050:8050 -p 8051:8051 scrapinghub/splash


### 安装scrapy-splash ###
	pip install scrapy-splash


### 在settings下配置 ###

    Splash服务器地址
    SPLASH_URL = 'http://localhost:8050'
    开启Splash的两个下载中间件并调整HttpCompressionMiddleware的次序
    DOWNLOADER_MIDDLEWARES = {
    'scrapy_splash.SplashCookiesMiddleware': 723,
    'scrapy_splash.SplashMiddleware': 725,
	'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
	}
    设置去重过滤器
    DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'
    用来支持cache_args（可选）
	SPIDER_MIDDLEWARES = {
	'scrapy_splash.SplashDeduplicateArgsMiddleware': 100,
	}


### spider编写 ###

	import scrapy
	from scrapy_splash import SplashRequest
	class QuotesSpider(scrapy.Spider):
		name = "quotes"
		allowed_domains = ["quotes.toscrape.com"]
		start_urls = ['http://quotes.toscrape.com/js/']
		def start_requests(self):
			for url in self.start_urls:
				yield SplashRequest(url, args={'images': 0, 'timeout': 3})
		def parse(self, response):
			for sel in response.css('div.quote'):
			quote = sel.css('span.text::text').extract_first()
			author = sel.css('small.author::text').extract_first()
			yield {'quote': quote, 'author': author}

## 三、SplashRequest构造器方法 ##


### url ###

	与scrapy.Request中的url相同，也就是待爬取页面的url

### headers ###

	与scrapy.Request中的headers相同。

### cookies ###

	与scrapy.Request中的cookies相同。

### args ###

	传递给Splash的参数（除url以外），如wait、timeout、images、js_source等。

